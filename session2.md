### 🧩 Session 2: Compute & Storage (EC2 & S3)

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
|8         |Launch and configure an EC2 instance|Instance Verification: After the instance is running, copy the public IP address into a web browser to verify that the Apache web server is correctly serving the "Hello World" page.|🔹1. Click on "Instances" in the left-hand menu and then click the "Launch Instance" button on the top right corner.<br> 🔹2. Give the instance a name<br> 🔹3. Select Amazon Linux AMI<br>🔹4. Choose the default t2.micro for general purpose<br>🔹5. Create a new keypair with the default settings and save it somewhere in your filesystem, f.e. in  `~/.ssh/name-of-your-key.pem`. Make sure to store the key pair file in a secure location, as it cannot be downloaded again.<br>🔹6. In the "Network Settings", create a new Security Group and allow SSH & HTTP traffic from anywhere.<br>🔹7. Configure the storage size with the default size.<br>🔹8. - Add Advanced details >> user data<br>`#!/bin/bash`<br>`# use this` `for` `your user data`<br>`# install httpd (linux version2)`<br>`yum update -y`<br>`yum install -y httpd`<br>`systemctl start httpd`<br>`systemctl enable httpd`<br>`echo "<h1>Hello World from $(hostname -f)</h1>"` `> /var/www/html/index.html`<br>🔹9. Click "Launch Instance".|
|9         |Troubleshooting Exercise 1|------|🔹1. **Security Group Rules**:<br>- Double-check your security group rules to ensure that HTTP (TCP port 80) and SSH (TCP port 22) are allowed from anywhere (`0.0.0.0/0`).<br>🔹2. **Instance Status**:<br>- Ensure the instance is in the "running" state. Check the AWS EC2 dashboard to confirm this.<br>🔹3. **User Data Script**:<br>- Sometimes, the user data script might fail or not run correctly. To check this:<br>🔹1. SSH into your instance using the key pair you provided during the setup.<br>        🔹2. Check if Apache (`httpd`) is installed: `rpm -q httpd`.<br>        🔹3. If Apache is installed, check if it's running: `systemctl status httpd`.<br>        🔹4. If Apache isn't running, try starting it manually: `sudo systemctl start httpd`.<br>        🔹5. Also, check if the index.html file was created in `/var/www/html/`.<br>🔹4. **Instance Logs**:<br>- Check the EC2 instance's system logs. This can be accessed from the EC2 Dashboard by selecting the instance, choosing the "Actions" drop-down menu, selecting "Instance Settings", and then "Get System Log". This might give you clues if something went wrong during startup.<br>After you've checked or performed each of these steps, try accessing the public IP in your web browser again. http://the-public-ip-of-your-instance|
|10        |SSH into EC2 instance|Hands-on exercise: Connect to an EC2 instance using SSH AWS Infrastructure SSH Access|🔹1. Put the correct permissions to your private key `chmod 400 ~/.ssh/name-of-your-key.pem`<br>🔹2. SSH to your instance, f.e. if you saved your key as `~/.ssh/name-of-your-key.pem` then you can log into your instance using its public IP with: `ssh -i ~/.ssh/name-of-your-key.pem ec2-user@XX.XX.XX.XXX`|
|11        |Attach EBS Volume to EC2| Practice with various settings such as delete storage on termination of instance.|🔹1. Open the Amazon EC2 console and navigate to the "Volumes" section.<br>🔹2. Click "Create Volume" and configure the volume settings, such as size, volume type, and availability zone. **Make sure the availability zone is the same as the machine you want to attach the disk**.<br>🔹3. Once the volume is created, select it and choose "Attach Volume" from the actions menu.<br>🔹4. In the attach volume dialog, select the instance to attach the volume to and specify the device name.<br>🔹5. Click "Attach" to attach the EBS volume to the EC2 instance.<br>🔹6. SSH into the instance and issue `lsblk` to see that the new volume appears in the specified device path. **TO-DO: format & mount disk**|

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
|13         |	Snapshots and Recycle Bin|REPLICATE SNAPSHOT INTO DIFFERENT REGION<br>RE-CREATE VOLUME FROM SNAPSHOT<br>RECYCLE BIN - RETENTION RULE|Use EBS snapshot features to back up and restore volumes. Try region-to-region copy and test retention rules.|
|14         |Troubleshooting Scenario 1|### Scenario 1: Slow Disk Performance Problem: Users are experiencing slow disk performance on an EC2 instance with an attached EBS volume. Troubleshooting Steps:  <br>🔹1. Check the instance type: Verify if the instance type is appropriate for the workload's IOPS and throughput requirements.  <br>🔹2. Review EBS volume type: Ensure that the EBS volume type is suitable for the workload. Consider upgrading to a higher performance volume type, such as Provisioned IOPS SSD (io1/io2), if necessary.  <br>🔹3. Monitor resource utilisation: Use CloudWatch or other monitoring tools to check if the CPU, memory, or network utilisation on the instance is high, which could impact disk performance.  <br>🔹4. Check I/O load: Analyse the I/O load on the EBS volume. If it exceeds the volume's performance limits, consider redistributing the workload across multiple volumes or upgrading to a higher performance volume.  <br>🔹5. Review instance-level issues: Look for any other potential issues at the instance level, such as misconfigured applications or bottlenecks in the network stack.|🔧 Scenario 1: Simulate “Slow Disk Performance”   <br>Instead of actual latency:   <br>Give them a CloudWatch chart screenshot showing high queue depth or throttling on a gp2 volume.   <br>Ask: “What’s wrong here? What would you recommend?”   <br>Bonus: Provide 2-3 EC2 instance types in a table and let them choose a better one for IOPS-heavy workloads.   <br>You can also demo how to:   <br>Switch from gp2 to io1/io2.   <br>Show where to find EBS metrics in CloudWatch.|
|15         |Troubleshooting Scenario 2|### Scenario 2: Volume Detachment Failure Problem: You are unable to detach an EBS volume from an EC2 instance. Troubleshooting Steps:  <br>🔹1. Check instance state: Verify that the instance is in a stopped state before attempting to detach the volume. Detaching a volume from a running or pending instance is not allowed.  <br>🔹2. Review attached devices: Ensure that there are no active processes or open files accessing the volume. Check if any applications or services are actively using the volume.  <br>🔹3. Check permissions: Make sure that the IAM user or role has the necessary permissions to detach the volume. Verify the user's permissions and IAM policies.  <br>🔹4. Review volume state: Check if the volume is in a healthy state and not experiencing any issues. If the volume is in an error state, troubleshoot the specific error and take appropriate actions.  <br>🔹5. Retry detachment: If all else fails, try detaching the volume again after a brief period. In some cases, a temporary issue might have caused the initial failure.|🔧 Scenario 2: Simulate “Detachment Failure”   <br>They can test this hands-on if you:   <br>Pre-create a volume, attach it to a running instance.   <br>Instruct them to detach it while it's in use or while the instance is still running — this will show a failure.   <br>You can also:   <br>Have them look up the IAM permissions required (ec2:DetachVolume)   <br>Include a CloudTrail event log snippet showing “unauthorizedOperation” or similar.|
|16         |Troubleshooting Scenario 3|### Scenario 3: Data Corruption Problem: Data corruption is observed on an EBS volume, leading to file system errors or data inconsistencies. Troubleshooting Steps:<br>🔹1. Check for underlying issues: Examine the EC2 instance and EBS volume for any potential issues, such as hardware failures or network connectivity problems.<br>🔹2. Validate file system integrity: Run file system consistency checks on the affected volume using tools like fsck (for Linux) or CHKDSK (for Windows). Address any identified file system errors.<br>🔹3. Analyse volume snapshots: If available, compare the corrupted volume's data with the data from previously created snapshots. Identify any changes or discrepancies that might have caused the corruption.<br>🔹4. Review application behaviour: Investigate if the data corruption is specific to a particular application or workload. Check for any misconfigured or faulty applications that could be causing the issue.<br>🔹5. Restore from backups: If necessary, restore the volume's data from a previous backup or snapshot. Ensure that the backup is not affected by the same data corruption issue.|🔧 Scenario 3: Simulate “Data Corruption”  <br>Corruption itself is hard to simulate, so simplify:  <br>Provide a broken Linux volume (via snapshot) they can mount and run fsck on.  <br>Or give them the output of a fsck run with errors and ask what steps they’d take.<br>Alternatively, simulate a misbehaving app writing junk data to a file repeatedly — then show them how to restore from snapshot.|
|17         |Hands-on exercise: Create an S3 bucket and upload files|------|🪣 🔹1. Create an S3 Bucket  <br>Objective: Create a new S3 bucket to store your files. <br>Steps:<br>Go to the AWS Management Console.<br>Navigate to the S3 service (found under “Storage”).<br>Click Create bucket.  <br>Enter a unique bucket name (globally unique across AWS).  <br>Leave the region as default (or choose one closest to you).  <br>Leave other settings as default for now (like ACLs, encryption).<br>Scroll down and click Create bucket.  <br>📁 🔹2. Upload Files to the Bucket<br>Objective: Upload one or more files to your new S3 bucket.<br>Steps:  <br>Click on the bucket name you just created.  <br>Click Upload > Add files.  <br>Select one or more files from your local machine.  <br>Click Upload at the bottom.|
|18         |Create a new bucket policy|------|🔐 🔹3. Make Files Public (Optional)  <br>Objective: Make your uploaded file publicly accessible.  <br>Steps<br>Select the file inside your bucket.  <br>Choose Actions > Make public (if available), or:  <br>Go to Permissions > Bucket Policy<br>Add a JSON policy like this:  <br>{  <br>"Version": "2012-10-17",  <br>"Statement": [{  <br>"Sid": "PublicReadGetObject",  <br>"Effect<br>"Allow",  <br>"Principal": "*",  <br>"Action": "s3:GetObject",  <br>"Resource": "arn:aws:s3:::your-bucket-name/*"<br>}]<br>}<br>Replace your-bucket-name with your actual bucket name.  <br>✅ Your file should now be accessible via URL:  <br>https://your-bucket-name.s3.amazonaws.com/your-file-name|
|19         |S3 Pre-signed URLs:|Objective: Share temporary access to a private file.  <br>Using AWS CLI:  <br>aws s3 presign s3://your-bucket-name/your-file-name <br> The resulting URL is valid for 1 hour by default.  <br>  Anyone with the URL can access the object until it expires.  <br>Useful for support staff or external users who need temporary access.|------|
|20         |S3 Versioning|Objective: Keep all versions of objects for recovery and rollback.  <br> Steps:  <br>Go to S3 > Your bucket > Properties tab. <br>Scroll to Bucket Versioning.  <br>Click Enable, then Save.  <br>Try uploading a file, editing it, then uploading again.  <br>Go to the Versions tab to see both versions.|------|
|21         |Website|🌍 6. Enable Static Website Hosting  <br>Objective: Turn your bucket into a public website.|Steps:  <br>Go to Properties > Static website hosting. <br> Click Edit, enable hosting.  <br>Set index document to index.html.  <br>Save changes and upload your HTML files<br>You’ll get a public URL like:  <br>http://your-bucket-name.s3-website-region.amazonaws.com  <br>🛑 Remember: The bucket must be public for website hosting to work.|
|22         |S3 Replication|Objective: Automatically replicate files from one bucket to another.|<br>🔹Prerequisites:  <br>Both source and destination buckets must have versioning enabled.  <br>🔹Steps:  <br>Create a second S3 bucket (destination).  <br>In the source bucket, go to Management > Replication rules.  <br>Click Create replication rule and follow the prompts:  <br>🔹Source: This bucket  <br>Destination: The new bucket  <br>🔹Permissions: IAM role will be created for you  <br>Upload a file to the source bucket and check if it appears in the destination.|
|23         |S3 Event notifications|Objective: Trigger an action (e.g., Lambda, SNS) when files are added.|<br>Steps:  <br>🔹Go to Properties > Event notifications.  <br>🔹Click Create event notification.  <br>🔹Name your event and choose:  <br>🔹Event type: e.g., All object create events  <br>🔹Destination: Lambda function, SNS topic, or SQS queue  <br>🔹Save and test by uploading a file|
|24         |CHECKPOINT|------|	🧹 Delete buckets and files.Disable replication rules or versioning if no longer needed.|
