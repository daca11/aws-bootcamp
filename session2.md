### 🧩 Session 2: Compute & Storage (EC2 & S3)

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
|8         |Launch and configure an EC2 instance|Instance Verification: After the instance is running, copy the public IP address into a web browser to verify that the Apache web server is correctly serving the "Hello World" page.|🔹1. Click on "Instances" in the left-hand menu and then click the "Launch Instance" button.<br>🔹2. Select Amazon Linux 2 default ami.<br>Choose the default t2.micro for general purpose<br>🔹3. In the "Configure Instance Details" step, leave the default settings and click "Next: Add Storage."<br>🔹4.In the "Select an existing key pair or create a new key pair" dialog, choose "Create a new key pair," provide a name (e.g., "MyEC2KeyPair"), and click "Download Key Pair." Make sure to store the key pair file (.pem for Linux or .ppk for Windows) in a secure location, as it cannot be downloaded again. Click "Launch Instances."<br>🔹5.Configure the storage size for your instance based on your requirements. For this exercise, the default size should suffice. Click "Next: Add Tags."<br>**Add a tag with the key "Name" and a value that describes your instance (e.g., "MyFirstEC2Instance"). Click "Next: Configure Security Group."<br>🔹6. - Add Advanced details >> user data<br>`#!/bin/bash`<br>`# use this` `for` `your user data`<br>`# install httpd (linux version2)`<br>`yum update -y`<br>`yum install -y httpd`<br>`systemctl start httpd`<br>`systemctl enable httpd`<br>`echo "<h1>Hello World from $(hostname -f)</h1>"` `> /var/www/html/index.html`<br>🔹7. -Configure a new security group with the following rules:<br>- For Linux instances: Allow SSH (TCP port 22) from your IP address.<br>- For Windows instances: Allow RDP (TCP port 3389) from your IP address.<br>- Optional: Allow HTTP (TCP port 80) and HTTPS (TCP port 443) from anywhere (0.0.0.0/0) if you plan to host a web application. Click "Review and Launch.|
|9         |Troubleshooting Exercise 1|------|🔹1. **Security Group Rules**:<br>- Double-check your security group rules to ensure that HTTP (TCP port 80) is allowed from your IP or anywhere (`0.0.0.0/0`).<br>🔹2. **Instance Status**:<br>- Ensure the instance is in the "running" state. Check the AWS EC2 dashboard to confirm this.<br>🔹3. **User Data Script**:<br>- Sometimes, the user data script might fail or not run correctly. To check this:<br>🔹1. SSH into your instance using the key pair you provided during the setup.<br>        🔹2. Check if Apache (`httpd`) is installed: `rpm -q httpd`.<br>        🔹3. If Apache is installed, check if it's running: `systemctl status httpd`.<br>        🔹4. If Apache isn't running, try starting it manually: `sudo systemctl start httpd`.<br>        🔹5. Also, check if the index.html file was created in `/var/www/html/`.<br>🔹4. **Instance Logs**:<br>- Check the EC2 instance's system logs. This can be accessed from the EC2 Dashboard by selecting the instance, choosing the "Actions" drop-down menu, selecting "Instance Settings", and then "Get System Log". This might give you clues if something went wrong during startup.<br>🔹5. **AMI Compatibility**:<br>- Ensure that the AMI you've chosen is compatible with the user data script. For instance, the provided script is tailored for Amazon Linux 2. If you've chosen a different Linux distribution or version, the package management commands or service names could be different.<br>🔹6. **Network ACLs**:<br>- While less common, ensure that the Network Access Control Lists (NACLs) associated with your EC2 instance's subnet aren't blocking incoming or outgoing HTTP traffic.<br>🔹7. **VPC Configuration**:<br>- Ensure that the VPC in which the EC2 instance is launched has an Internet Gateway attached and that the instance's subnet route table has a route pointing to this Internet Gateway. This ensures that the instance can communicate with the internet.<br>After you've checked or performed each of these steps, try accessing the public IP in your web browser again. http://xxxxx|
|10        |SSH into EC2 instance|Hands-on exercise: Connect to an EC2 instance using SSH AWS Infrastructure SSH Access|`ssh -i ~/path/to/your/private/key.pem ec2-user@XX.XX.XX.XXX`<br>`#Public IPv4 address or Public IPv4 DNS`<br>`#e.g. ssh -i ~/Documents/projects/aws/aws-demo/ec2-demo1.pem ec2-user@54.247.10.9`|
|11        |Troubleshooting SSH access issues|chmod 400` `your_private_key.pem`<br> --- <br>`ssh -v -i /path/to/your_private_key.pem ec2-user@XX.XX.XX.XXX`|**Test SSH permissions**<br> `aws iam list-users`|
|12        |Attach EBS Volume to EC2|- practice with various settings such as delete storage on termination of instance.|🔹1. Open the Amazon EC2 console and navigate to the "Volumes" section.<br>🔹2. Click "Create Volume" and configure the volume settings, such as size, volume type, and availability zone.<br>🔹3. Once the volume is created, select it and choose "Attach Volume" from the actions menu.<br>🔹4. In the attach volume dialog, select the instance to attach the volume to and specify the device name.<br>🔹5. Click "Attach" to attach the EBS volume to the EC2 instance.|
|13         |	Snapshots and Recycle Bin|REPLICATE SNAPSHOT INTO DIFFERENT REGION<br>RE-CREATE VOLUME FROM SNAPSHOT<br>RECYCLE BIN - RETENTION RULE|Use EBS snapshot features to back up and restore volumes. Try region-to-region copy and test retention rules.|
|14         |Troubleshooting Scenario 1|### Scenario 1: Slow Disk Performance Problem: Users are experiencing slow disk performance on an EC2 instance with an attached EBS volume. Troubleshooting Steps:  <br>🔹1. Check the instance type: Verify if the instance type is appropriate for the workload's IOPS and throughput requirements.  <br>🔹2. Review EBS volume type: Ensure that the EBS volume type is suitable for the workload. Consider upgrading to a higher performance volume type, such as Provisioned IOPS SSD (io1/io2), if necessary.  <br>🔹3. Monitor resource utilisation: Use CloudWatch or other monitoring tools to check if the CPU, memory, or network utilisation on the instance is high, which could impact disk performance.  <br>🔹4. Check I/O load: Analyse the I/O load on the EBS volume. If it exceeds the volume's performance limits, consider redistributing the workload across multiple volumes or upgrading to a higher performance volume.  <br>🔹5. Review instance-level issues: Look for any other potential issues at the instance level, such as misconfigured applications or bottlenecks in the network stack.|🔧 Scenario 1: Simulate “Slow Disk Performance”   <br>Instead of actual latency:   <br>Give them a CloudWatch chart screenshot showing high queue depth or throttling on a gp2 volume.   <br>Ask: “What’s wrong here? What would you recommend?”   <br>Bonus: Provide 2-3 EC2 instance types in a table and let them choose a better one for IOPS-heavy workloads.   <br>You can also demo how to:   <br>Switch from gp2 to io1/io2.   <br>Show where to find EBS metrics in CloudWatch.|
|15         |Troubleshooting Scenario 2|### Scenario 2: Volume Detachment Failure Problem: You are unable to detach an EBS volume from an EC2 instance. Troubleshooting Steps:  <br>🔹1. Check instance state: Verify that the instance is in a stopped state before attempting to detach the volume. Detaching a volume from a running or pending instance is not allowed.  <br>🔹2. Review attached devices: Ensure that there are no active processes or open files accessing the volume. Check if any applications or services are actively using the volume.  <br>🔹3. Check permissions: Make sure that the IAM user or role has the necessary permissions to detach the volume. Verify the user's permissions and IAM policies.  <br>🔹4. Review volume state: Check if the volume is in a healthy state and not experiencing any issues. If the volume is in an error state, troubleshoot the specific error and take appropriate actions.  <br>🔹5. Retry detachment: If all else fails, try detaching the volume again after a brief period. In some cases, a temporary issue might have caused the initial failure.|🔧 Scenario 2: Simulate “Detachment Failure”   <br>They can test this hands-on if you:   <br>Pre-create a volume, attach it to a running instance.   <br>Instruct them to detach it while it's in use or while the instance is still running — this will show a failure.   <br>You can also:   <br>Have them look up the IAM permissions required (ec2:DetachVolume)   <br>Include a CloudTrail event log snippet showing “unauthorizedOperation” or similar.|
|16         |Troubleshooting Scenario 3|### Scenario 3: Data Corruption Problem: Data corruption is observed on an EBS volume, leading to file system errors or data inconsistencies. Troubleshooting Steps:<br>🔹1. Check for underlying issues: Examine the EC2 instance and EBS volume for any potential issues, such as hardware failures or network connectivity problems.<br>🔹2. Validate file system integrity: Run file system consistency checks on the affected volume using tools like fsck (for Linux) or CHKDSK (for Windows). Address any identified file system errors.<br>🔹3. Analyse volume snapshots: If available, compare the corrupted volume's data with the data from previously created snapshots. Identify any changes or discrepancies that might have caused the corruption.<br>🔹4. Review application behaviour: Investigate if the data corruption is specific to a particular application or workload. Check for any misconfigured or faulty applications that could be causing the issue.<br>🔹5. Restore from backups: If necessary, restore the volume's data from a previous backup or snapshot. Ensure that the backup is not affected by the same data corruption issue.|🔧 Scenario 3: Simulate “Data Corruption”  <br>Corruption itself is hard to simulate, so simplify:  <br>Provide a broken Linux volume (via snapshot) they can mount and run fsck on.  <br>Or give them the output of a fsck run with errors and ask what steps they’d take.<br>Alternatively, simulate a misbehaving app writing junk data to a file repeatedly — then show them how to restore from snapshot.|
|17         |Hands-on exercise: Create an S3 bucket and upload files|------|🪣 🔹1. Create an S3 Bucket  <br>Objective: Create a new S3 bucket to store your files. <br>Steps:<br>Go to the AWS Management Console.<br>Navigate to the S3 service (found under “Storage”).<br>Click Create bucket.  <br>Enter a unique bucket name (globally unique across AWS).  <br>Leave the region as default (or choose one closest to you).  <br>Leave other settings as default for now (like ACLs, encryption).<br>Scroll down and click Create bucket.  <br>📁 🔹2. Upload Files to the Bucket<br>Objective: Upload one or more files to your new S3 bucket.<br>Steps:  <br>Click on the bucket name you just created.  <br>Click Upload > Add files.  <br>Select one or more files from your local machine.  <br>Click Upload at the bottom.|
|18         |Create a new bucket policy|------|🔐 🔹3. Make Files Public (Optional)  <br>Objective: Make your uploaded file publicly accessible.  <br>Steps<br>Select the file inside your bucket.  <br>Choose Actions > Make public (if available), or:  <br>Go to Permissions > Bucket Policy<br>Add a JSON policy like this:  <br>{<br>  "Version": "2012-10-17",<br>  "Statement": [<br>    {<br>      "Sid": "PublicReadGetObject",<br>      "Effect": "Allow",<br>      "Principal": "*",<br>     "Action": "s3:GetObject",<br>     "Resource": "arn:aws:s3:::demo-bucket-aose/*" <br>    }<br>  ]<br>}<br>Replace your-bucket-name with your actual bucket name.  <br>✅ Your file should now be accessible via URL:  <br>https://your-bucket-name.s3.amazonaws.com/your-file-name|
|19         |S3 Pre-signed URLs:|Objective: Share temporary access to a private file.  <br>Using AWS CLI:  <br>aws s3 presign s3://your-bucket-name/your-file-name <br> The resulting URL is valid for 1 hour by default.  <br>  Anyone with the URL can access the object until it expires.  <br>Useful for support staff or external users who need temporary access.|------|
|20         |S3 Versioning|Objective: Keep all versions of objects for recovery and rollback.  <br> Steps:  <br>Go to S3 > Your bucket > Properties tab. <br>Scroll to Bucket Versioning.  <br>Click Enable, then Save.  <br>Try uploading a file, editing it, then uploading again.  <br>Go to the Versions tab to see both versions.|------|
|21         |Website|🌍 6. Enable Static Website Hosting  <br>Objective: Turn your bucket into a public website.|Steps:  <br>Go to Properties > Static website hosting. <br> Click Edit, enable hosting.  <br>Set index document to index.html.  <br>Save changes and upload your HTML files<br>You’ll get a public URL like:  <br>http://your-bucket-name.s3-website-region.amazonaws.com  <br>🛑 Remember: The bucket must be public for website hosting to work.|
|22         |S3 Replication|Objective: Automatically replicate files from one bucket to another.|<br>🔹Prerequisites:  <br>Both source and destination buckets must have versioning enabled.  <br>🔹Steps:  <br>Create a second S3 bucket (destination).  <br>In the source bucket, go to Management > Replication rules.  <br>Click Create replication rule and follow the prompts:  <br>🔹Source: This bucket  <br>Destination: The new bucket  <br>🔹Permissions: IAM role will be created for you  <br>Upload a file to the source bucket and check if it appears in the destination.|
|23         |S3 Event notifications|Objective: Trigger an action (e.g., Lambda, SNS) when files are added.*****MAY HAVE PERMISSIONS ISSUES|<br>Steps:  <br>🔹Go to Properties > Event notifications.  <br>🔹Click Create event notification.  <br>🔹Name your event and choose:  <br>🔹Event type: e.g., All object create events  <br>🔹Destination: Lambda function, SNS topic, or SQS queue  <br>🔹Save and test by uploading a file|
|24         |CHECKPOINT|------|	🧹 Delete buckets and files.Disable replication rules or versioning if no longer needed.|
