# aws-essentials

A workshop

#AWS Account
https://730492949633.signin.aws.amazon.com/console

# AWS Region
eu-west-1 Ireland

| **Team**       | **Users**      | **Users**     | **Users** | **Users** |
|----------------|----------------|---------------|-----------|-----------|
| Admins         | soyred_admin   |               |           |           |
| Team 1         | Team1-User1    | Team1-User2   |  etc      |    etc    |
| Team 2         | Team2-User1    | Team2-User2   |  etc      |    etc    |

### CIDR Blocks, Availability Zones, Subnets and Route Tables

| **Team**       | **CIDR Block**  | **Availability Zone** | **Public Subnet**     | **Private Subnet**    | **Public Route Table**                | **Private Route Table**                            |
|----------------|-----------------|-----------------------|-----------------------|-----------------------|---------------------------------------|----------------------------------------------------|
| Demo Account   | 10.0.0.0/16     | eu-west-1c            | 10.0.0.0/24           | 10.0.16.0/20          | 0.0.0.0/0 via Internet Gateway        | 0.0.0.0/0 via NAT Instance/Gateway or VPC Endpoint |
| Team 1         | 10.2.0.0/16     | eu-west-1a            | 10.2.0.0/24           | 10.2.16.0/20          | 0.0.0.0/0 via Internet Gateway        | 0.0.0.0/0 via NAT Instance/Gateway or VPC Endpoint |
| Team 2         | 10.3.0.0/16     | eu-west-1b            | 10.3.0.0/24           | 10.3.16.0/20          | 0.0.0.0/0 via Internet Gateway        | 0.0.0.0/0 via NAT Instance/Gateway or VPC Endpoint |
| Team 3         | 10.4.0.0/16     | eu-west-1c            | 10.4.0.0/24           | 10.4.16.0/20          | 0.0.0.0/0 via Internet Gateway        | 0.0.0.0/0 via NAT Instance/Gateway or VPC Endpoint |
| Additional     | 10.5.0.0/16     |                       | 10.5.0.0/24           | 10.5.16.0/20          | 0.0.0.0/0 via Internet Gateway        | 0.0.0.0/0 via NAT Instance/Gateway or VPC Endpoint |


### Security Group Rules

| **Team**       | **Security Group**    | **Inbound Rules**                                                                                                                      | **Outbound Rules**            |
|----------------|-----------------------|----------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|
| All Accounts   | Public SG             | Allow SSH (22) from your IP (Restricts EC2 Connect) & Allow HTTP 80 from  (0.0.0.0/0)                                                  | Allow all traffic (0.0.0.0/0) |
|                | Private SG            | Allow SSH (22) from Public SG/Private IP of Public aka Bastion Host                                                                    | Allow all traffic (0.0.0.0/0) |
|                | Nat SG                | Allow HTTP (80) from VPC CIDR<br>Allow HTTPS (443) from VPC CIDR<br>Allow SSH (22) from VPC CIDR<br>Allow ALL ICMP-IPv4 from 0.0.0.0/0 | Allow all traffic (0.0.0.0/0) |

### üß© Session 1: Foundations & IAM

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
|0         |**Prerequisites for Facilitators:**<br>SSH key pairs (or guide for generating)<br>Example CSV file for Athena exercises<br>Sample IAM policy templates<br>User data scripts for EC2 auto-installation<br>Pre-created VPC with private and public subnets (or use default)<br>Quick reference sheet / Cheatsheet PDF<br>Enable AWS CloudTrail **before** the workshop"|SHOW THE CLASS - - <br>Enable AWS CloudTrail **before** the workshop|1. **Go to the CloudTrail Console**<br>https://console.aws.amazon.com/cloudtrail<br>2. **Choose**: `Trails` from the sidebar<br>- If none exists, click **"Create trail"**<br>3. **Trail Name**:<br>- Example: `WorkshopTrail`<br>4. **Storage Location**:<br>- Choose or create an **S3 bucket** to store logs (e.g. `aws-workshop-cloudtrail-logs`)<br>5. **Apply Trail To All ‚úÖ<br>- Recommended: Ensures activity is tracked even if students switch regions.<br>6. **Management Events**:<br>- Enable **Read and Write events**<br>7. Insight Events (optional)**- Helps with detecting unusual activity (good for longer workshops or debugging)<br>8. **Create Trail**<br>üìù **Note**: CloudTrail starts recording immediately but events may take a few minutes to show in the logs.
|1         |Login to AWS Console|Change console theme (dark/light)<br>Add bookmarks for: IAM, EC2, S3, CloudWatch<br>Explore service search, region selection|Participants should be comfortable navigating the console, using bookmarks to access frequently used services, and adjusting the theme to their preference.|
|2          |Check two users within the IAM demo groups:<br>[Developers Group]<br>[QA Group]|Examine the policies attached to each group.<br>Simulate each user's access to various resources using the IAM Policy Simulator.|Developers should have permissions to **read/write to AWS CodeCommit** and deploy code to **Elastic Beanstalk or EC2** QA Engineers should have **read-only access to CodeCommit**, and permissions to view logs in **CloudWatch** and **X-Ray** for troubleshooting.|
|3         |Use IAM Policy Simulator|Simulate each user's access to various resources using the **IAM Policy Simulator**.|For Developers: Verify if they can **push code** to the repository.<br>For QA Engineers: Verify if they can access **CloudWatch logs** and troubleshoot issues.|
|4         |Check ANSWER|Check policies|Developers<br>- `AWSCodeCommitReadOnly`: Allows read access to the code repository in AWS CodeCommit.<br>- `AWSCodeDeployFullAccess`: Allows developers to deploy the code using AWS CodeDeploy.<br>- `AmazonS3FullAccess`: Enables read and write access to the S3 buckets related to the project.<br>- *Custom policy for Elastic Beanstalk or EC2 instances deployment, which allows developers to create, update, and delete resources related to the application.*<br>QA Engineers:<br>- `AWSCodeCommitReadOnly`: Allows read access to the code repository in AWS CodeCommit.<br>- `AmazonS3ReadOnlyAccess`: Enables read access to the S3 buckets related to the project.<br>- `CloudWatchReadOnlyAccess`: Allows QA engineers to access logs from Amazon CloudWatch.<br>- `AWSXRayReadOnlyAccess`: Provides read access to AWS X-Ray to help QA engineers in troubleshooting and performance analysis.|
|5         |Create a Role|- Create a new **IAM Role** that allows EC2 instances to assume the role for accessing S3.<br>- Select **EC2** as the trusted entity.<br>- Attach a policy like `AmazonS3ReadOnlyAccess` to allow the EC2 instance to read from S3.|When launching or modifying an EC2 instance, choose the IAM Role created earlier to grant it access to the necessary resources.|
|6         |Create Access keys for an user|- **Create Access Keys**:<br>- Use the IAM console to create **access keys** for the non-admin user.<br>- Copy the **access key** and **secret key** and configure them using the AWS CLI by running:<br>- `aws configure`<br>- Enter the Access Key and Secret Key when prompted.|- **Verify CLI Configuration**:<br>- Test the configuration by running:<br>- `aws iam list-users`<br>- The user should only have access to the **permissions granted to their IAM group**.|
|7         |CHECKPOINT|-----------------|After completing this exercise, participants should be able to:<br>- Understand the purpose of IAM users, groups, roles, and policies.<br>- Use the **IAM Policy Simulator** to test and validate permissions.<br>- Apply the **least privilege principle** when assigning permissions.|

### üß© Session 2: Compute & Storage (EC2 & S3)

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
|8         |Launch and configure an EC2 instance|1. Click on "Instances" in the left-hand menu and then click the "Launch Instance" button.<br>2. Select Amazon Linux 2 default ami.<br>Choose the default t2.micro for general purpose<br>3. In the "Configure Instance Details" step, leave the default settings and click "Next: Add Storage."<br>4.In the "Select an existing key pair or create a new key pair" dialog, choose "Create a new key pair," provide a name (e.g., "MyEC2KeyPair"), and click "Download Key Pair." Make sure to store the key pair file (.pem for Linux or .ppk for Windows) in a secure location, as it cannot be downloaded again. Click "Launch Instances."<br>5.Configure the storage size for your instance based on your requirements. For this exercise, the default size should suffice. Click "Next: Add Tags."<br>**Add a tag with the key "Name" and a value that describes your instance (e.g., "MyFirstEC2Instance"). Click "Next: Configure Security Group."<br>6. - Add Advanced details >> user data<br>`#!/bin/bash`<br>`# use¬†this`¬†`for`¬†`your user data`<br>`# install httpd (linux version2)`<br>`yum update -y`<br>`yum install -y httpd`<br>`systemctl start httpd`<br>`systemctl enable httpd`<br>`echo¬†"<h1>Hello World from $(hostname -f)</h1>"`¬†`> /var/www/html/index.html`<br>7. -Configure a new security group with the following rules:<br>- For Linux instances: Allow SSH (TCP port 22) from your IP address.<br>- For Windows instances: Allow RDP (TCP port 3389) from your IP address.<br>- Optional: Allow HTTP (TCP port 80) and HTTPS (TCP port 443) from anywhere (0.0.0.0/0) if you plan to host a web application. Click "Review and Launch.|Instance Verification: After the instance is running, copy the public IP address into a web browser to verify that the Apache web server is correctly serving the "Hello World" page.|
|9         |Troubleshooting Exercise 1|1. **Security Group Rules**:<br>- Double-check your security group rules to ensure that HTTP (TCP port 80) is allowed from your IP or anywhere (`0.0.0.0/0`).<br>2. **Instance Status**:<br>- Ensure the instance is in the "running" state. Check the AWS EC2 dashboard to confirm this.<br>3. **User Data Script**:<br>- Sometimes, the user data script might fail or not run correctly. To check this:<br>1. SSH into your instance using the key pair you provided during the setup.<br>        2. Check if Apache (`httpd`) is installed:¬†`rpm -q httpd`.<br>        3. If Apache is installed, check if it's running:¬†`systemctl status httpd`.<br>        4. If Apache isn't running, try starting it manually:¬†`sudo systemctl start httpd`.<br>        5. Also, check if the index.html file was created in¬†`/var/www/html/`.<br>4. **Instance Logs**:<br>- Check the EC2 instance's system logs. This can be accessed from the EC2 Dashboard by selecting the instance, choosing the "Actions" drop-down menu, selecting "Instance Settings", and then "Get System Log". This might give you clues if something went wrong during startup.<br>5. **AMI Compatibility**:<br>- Ensure that the AMI you've chosen is compatible with the user data script. For instance, the provided script is tailored for Amazon Linux 2. If you've chosen a different Linux distribution or version, the package management commands or service names could be different.<br>6. **Network ACLs**:<br>- While less common, ensure that the Network Access Control Lists (NACLs) associated with your EC2 instance's subnet aren't blocking incoming or outgoing HTTP traffic.<br>7. **VPC Configuration**:<br>- Ensure that the VPC in which the EC2 instance is launched has an Internet Gateway attached and that the instance's subnet route table has a route pointing to this Internet Gateway. This ensures that the instance can communicate with the internet.<br>After you've checked or performed each of these steps, try accessing the public IP in your web browser again. http://xxxxx|------|
|10        |Hands-on exercise: Connect to an EC2 instance using SSH AWS Infrastructure SSH Access|`ssh -i ~/path/to/your/private/key.pem ec2-user@XX.XX.XX.XXX`<br>`#Public IPv4 address or Public IPv4 DNS`<br>`#e.g. ssh -i ~/Documents/projects/aws/aws-demo/ec2-demo1.pem ec2-user@54.247.10.9`|------|
|11        |Troubleshooting SSH access issues|chmod¬†400`¬†`your_private_key.pem`<br> --- <br>`ssh -v -i /path/to/your_private_key.pem ec2-user@XX.XX.XX.XXX`|**Test SSH permissions**<br> `aws iam list-users`|
|12        |Exercise: Create and attach an EBS volume to an EC2 instance|1. Open the Amazon EC2 console and navigate to the "Volumes" section.<br>2. Click "Create Volume" and configure the volume settings, such as size, volume type, and availability zone.<br>3. Once the volume is created, select it and choose "Attach Volume" from the actions menu.<br>4. In the attach volume dialog, select the instance to attach the volume to and specify the device name.<br>5. Click "Attach" to attach the EBS volume to the EC2 instance.|- practice with various settings such as delete storage on termination of instance.|
|13         |- REPLICATE SNAPSHOT INTO DIFFERENT REGION <br>- RE-CREATE VOLUME FROM SNAPSHOT <br>- RECYCLE BIN - RETENTION RULE|-----------------|------|
|14         |Troubleshooting Scenario 1|### Scenario 1: Slow Disk Performance Problem: Users are experiencing slow disk performance on an EC2 instance with an attached EBS volume. Troubleshooting Steps:  <br>1. Check the instance type: Verify if the instance type is appropriate for the workload's IOPS and throughput requirements.  <br>2. Review EBS volume type: Ensure that the EBS volume type is suitable for the workload. Consider upgrading to a higher performance volume type, such as Provisioned IOPS SSD (io1/io2), if necessary.  <br>3. Monitor resource utilisation: Use CloudWatch or other monitoring tools to check if the CPU, memory, or network utilisation on the instance is high, which could impact disk performance.  <br>4. Check I/O load: Analyse the I/O load on the EBS volume. If it exceeds the volume's performance limits, consider redistributing the workload across multiple volumes or upgrading to a higher performance volume.  <br>5. Review instance-level issues: Look for any other potential issues at the instance level, such as misconfigured applications or bottlenecks in the network stack.|üîß Scenario 1: Simulate ‚ÄúSlow Disk Performance‚Äù   <br>Instead of actual latency:   <br>Give them a CloudWatch chart screenshot showing high queue depth or throttling on a gp2 volume.   <br>Ask: ‚ÄúWhat‚Äôs wrong here? What would you recommend?‚Äù   <br>Bonus: Provide 2-3 EC2 instance types in a table and let them choose a better one for IOPS-heavy workloads.   <br>You can also demo how to:   <br>Switch from gp2 to io1/io2.   <br>Show where to find EBS metrics in CloudWatch.|
|15         |Troubleshooting Scenario 2|### Scenario 2: Volume Detachment Failure Problem: You are unable to detach an EBS volume from an EC2 instance. Troubleshooting Steps:  <br>1. Check instance state: Verify that the instance is in a stopped state before attempting to detach the volume. Detaching a volume from a running or pending instance is not allowed.  <br>2. Review attached devices: Ensure that there are no active processes or open files accessing the volume. Check if any applications or services are actively using the volume.  <br>3. Check permissions: Make sure that the IAM user or role has the necessary permissions to detach the volume. Verify the user's permissions and IAM policies.  <br>4. Review volume state: Check if the volume is in a healthy state and not experiencing any issues. If the volume is in an error state, troubleshoot the specific error and take appropriate actions.  <br>5. Retry detachment: If all else fails, try detaching the volume again after a brief period. In some cases, a temporary issue might have caused the initial failure.|üîß Scenario 2: Simulate ‚ÄúDetachment Failure‚Äù   <br>They can test this hands-on if you:   <br>Pre-create a volume, attach it to a running instance.   <br>Instruct them to detach it while it's in use or while the instance is still running ‚Äî this will show a failure.   <br>You can also:   <br>Have them look up the IAM permissions required (ec2:DetachVolume)   <br>Include a CloudTrail event log snippet showing ‚ÄúunauthorizedOperation‚Äù or similar.|
|16         |Troubleshooting Scenario 3|### Scenario 3: Data Corruption Problem: Data corruption is observed on an EBS volume, leading to file system errors or data inconsistencies. Troubleshooting Steps:<br>1. Check for underlying issues: Examine the EC2 instance and EBS volume for any potential issues, such as hardware failures or network connectivity problems.<br>2. Validate file system integrity: Run file system consistency checks on the affected volume using tools like fsck (for Linux) or CHKDSK (for Windows). Address any identified file system errors.<br>3. Analyse volume snapshots: If available, compare the corrupted volume's data with the data from previously created snapshots. Identify any changes or discrepancies that might have caused the corruption.<br>4. Review application behaviour: Investigate if the data corruption is specific to a particular application or workload. Check for any misconfigured or faulty applications that could be causing the issue.<br>5. Restore from backups: If necessary, restore the volume's data from a previous backup or snapshot. Ensure that the backup is not affected by the same data corruption issue.|üîß Scenario 3: Simulate ‚ÄúData Corruption‚Äù  <br>Corruption itself is hard to simulate, so simplify:  <br>Provide a broken Linux volume (via snapshot) they can mount and run fsck on.  <br>Or give them the output of a fsck run with errors and ask what steps they‚Äôd take.<br>Alternatively, simulate a misbehaving app writing junk data to a file repeatedly ‚Äî then show them how to restore from snapshot.|
|17         |Hands-on exercise: Create an S3 bucket and upload files|ü™£ 1. Create an S3 Bucket  <br>Objective: Create a new S3 bucket to store your files. <br>Steps:<br>Go to the AWS Management Console.<br>Navigate to the S3 service (found under ‚ÄúStorage‚Äù).<br>Click Create bucket.  <br>Enter a unique bucket name (globally unique across AWS).  <br>Leave the region as default (or choose one closest to you).  <br>Leave other settings as default for now (like ACLs, encryption).<br>Scroll down and click Create bucket.  <br>üìÅ 2. Upload Files to the Bucket<br>Objective: Upload one or more files to your new S3 bucket.<br>Steps:  <br>Click on the bucket name you just created.  <br>Click Upload > Add files.  <br>Select one or more files from your local machine.  <br>Click Upload at the bottom.|------|
|18         |Create a new bucket policy|üîê 3. Make Files Public (Optional)  <br>Objective: Make your uploaded file publicly accessible.  <br>Steps<br>Select the file inside your bucket.  <br>Choose Actions > Make public (if available), or:  <br>Go to Permissions > Bucket Policy<br>Add a JSON policy like this:  <br>{  <br>"Version": "2012-10-17",  <br>"Statement": [{  <br>"Sid": "PublicReadGetObject",  <br>"Effect<br>"Allow",  <br>"Principal": "*",  <br>"Action": "s3:GetObject",  <br>"Resource": "arn:aws:s3:::your-bucket-name/*"<br>}]<br>}<br>Replace your-bucket-name with your actual bucket name.  <br>‚úÖ Your file should now be accessible via URL:  <br>https://your-bucket-name.s3.amazonaws.com/your-file-name|------|
|19         |S3 Pre-signed URLs:|Objective: Share temporary access to a private file.  <br>Using AWS CLI:  <br>aws s3 presign s3://your-bucket-name/your-file-name <br> The resulting URL is valid for 1 hour by default.  <br>  Anyone with the URL can access the object until it expires.  <br>Useful for support staff or external users who need temporary access.|------|
|20         |S3 Versioning|Objective: Keep all versions of objects for recovery and rollback.  <br> Steps:  <br>Go to S3 > Your bucket > Properties tab. <br>Scroll to Bucket Versioning.  <br>Click Enable, then Save.  <br>Try uploading a file, editing it, then uploading again.  <br>Go to the Versions tab to see both versions.|------|
|21         |Website|üåç 6. Enable Static Website Hosting  <br>Objective: Turn your bucket into a public website.  <br>Steps:  <br>Go to Properties > Static website hosting. <br> Click Edit, enable hosting.  <br>Set index document to index.html.  <br>Save changes and upload your HTML files<br>You‚Äôll get a public URL like:  <br>http://your-bucket-name.s3-website-region.amazonaws.com  <br>üõë Remember: The bucket must be public for website hosting to work.|------|
|22         |S3 Replication|Objective: Automatically replicate files from one bucket to another.  <br>Prerequisites:  <br>Both source and destination buckets must have versioning enabled.  <br>Steps:  <br>Create a second S3 bucket (destination).  <br>In the source bucket, go to Management > Replication rules.  <br>Click Create replication rule and follow the prompts:  <br>Source: This bucket  <br>Destination: The new bucket  <br>Permissions: IAM role will be created for you  <br>Upload a file to the source bucket and check if it appears in the destination.|-----|
|23         |S3 Event notifications|Objective: Trigger an action (e.g., Lambda, SNS) when files are added. <br>Steps:  <br>Go to Properties > Event notifications.  <br>Click Create event notification.  <br>Name your event and choose:  <br>Event type: e.g., All object create events  <br>Destination: Lambda function, SNS topic, or SQS queue  <br>Save and test by uploading a file.|------|
|24         |CHECKPOINT|Delete buckets and files after completing the exercise.  <br>Turn off replication rules or versioning if not needed.|------|

### üß© Session 3: Scaling & Load Balancing

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
|25        |AWS ALB deployment and flush setup|Objective: Deploy an ALB to route traffic to different EC2 instances based on ports (e.g., Author and Publisher AEM instances).|The load balancer now will listen to the traffic on port 80 (or 443 if you've set HTTPS), and route that traffic to the respective target groups (author or publisher) which are listening on their respective ports (4502 for author, 4503 for publisher). This ensures the standard web traffic received on port 80/443 is correctly routed to your AEM instances on their respective ports.<br>Also, remember to set up your security groups to allow traffic on these ports and restrict access to only trusted sources.|
|26       |-----------------|üõ† Steps:<br>Go to EC2 Dashboard in the AWS Console.<br>Under Load Balancing, select Load Balancers and click Create Load Balancer.<br>Choose Application Load Balancer.<br>Configure:<br>Name your ALB.<br>Scheme: Internet-facing.<br>Listeners: Add HTTP (port 80) and/or HTTPS (port 443).<br>If using HTTPS, assign an SSL certificate from ACM.<br>Select or create Security Groups that allow traffic on required ports.<br>Configure Routing:<br>Create two Target Groups:<br>One for the Author instance (HTTP on port 4502)<br>One for Publisher instance (HTTP on port 4503)<br>Register EC2 instances:<br>Add the Author instance to the 4502 target group.<br>Add Publisher instances to the 4503 target group.<br>Click Create and wait for the ALB to provision.<br>Once active, verify:ALB status is active.<br>Target health checks show healthy.|-----------------|
|27       |Setting up connection draining (flush setup)|üßπ 2. Configure Connection Draining (Deregistration Delay)  <br>Objective: Allow active connections to complete before an instance is removed from the ALB (graceful shutdown).  <br>üõ† Steps:  <br>In the EC2 Dashboard, go to Target Groups under Load Balancing.  <br>Select the target group you want to configure. <br>Click the Attributes tab.   <br>Click Edit attributes.   <br>Set Deregistration delay timeout (e.g., 600 seconds).  <br>Click Save.  <br>üß† This ensures clients aren‚Äôt disconnected during instance termination or scale-in events.|Once set, the ALB will allow existing requests to the de-registering instance to complete for the specified delay timeout, thereby ensuring smooth operation even during instance de-registration. It's important to note that this setting applies to all targets in the target group. If you need different settings for different targets, you would have to separate them into different target groups.  <br>Remember that while connection draining provides a way to ensure application availability during maintenance or failure events, it's only one part of a comprehensive high availability strategy. Other techniques like auto-scaling, multi-AZ deployment, and proactive application health monitoring should also be considered.|
|28       |Configure Auto Scaling for EC2 Instances|# 1. Step 1: Sign in to AWS Management Console and open the EC2 Dashboard   <br>First, sign in to your AWS Management Console. Once logged in, navigate to the EC2 Dashboard.  <br># 2. Step 2: Create a Launch Template  <br>A launch template will store the configuration for your EC2 instances.  <br>1. From the EC2 dashboard, click on "Launch Templates" under "INSTANCES" in the left navigation pane.  <br>2. Click on "Create launch template".  <br>3. Fill in the required details like the template name, description, Amazon Machine Image (AMI), instance type, key pair, security groups, and storage.  <br>4. Click on "Create launch template".  <br># 3. Step 3: Configure Auto Scaling Group  <br>### **Auto Scaling Components**  <br>AWS Auto Scaling revolves around three key components:  <br>- **Groups:**¬†These are the logical components of Auto Scaling, such as a group of EC2 instances for web-servers or a group of RDS instances for databases.  <br>- **Configuration Templates:**¬†Templates instruct Auto Scaling groups on how to configure and launch new instances that align with scaling requirements.  <br>- **Scaling Options:**¬†You have a plethora of ways to scale your Auto Scaling groups, either conditionally, scheduled, manually, or based on a predictive model.  |You can test the setup by artificially increasing the load on your instances (if you have set up a scale-out policy based on load) or by setting a scheduled action for a future time and checking the behaviour of the Auto Scaling group at the scheduled time.|
|29       |Auto scaling|Now, you'll create an Auto Scaling group which will use the launch template.  <br>1. From the EC2 dashboard, click on "Auto Scaling groups" under "AUTO SCALING" in the left navigation pane.  <br>2. Click on "Create Auto Scaling group".  <br>3. Choose "Launch template" as your launch type and select the template you created earlier.  <br>4. Click on "Next".  <br>5. Choose your VPC and the subnets where your instances should be launched. Click on "Next".  <br>6. Specify the group size. You can set the desired, minimum and maximum number of instances for your Auto Scaling group. Click on "Next".  <br>7. You can optionally set up scaling policies to increase or decrease the number of instances based on load or schedule. Click on "Next".  <br>8. Add any necessary tags, then click on "Next".  <br>9. Review your settings and click on "Create Auto Scaling group".  <br>Congratulations! You've now set up an Auto Scaling group for your EC2 instances.|Remember that the Auto Scaling group might take a few minutes to launch new instances when the conditions for scale-out are met, and it might take a few minutes to terminate instances when scaling-in.|
|30       |CHECKPOINT|delete.... close down... |------|

### üß© Session 4: Databases & Analytics

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
