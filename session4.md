### 🧩 Session 4: Databases & Analytics

| **#** | **step** | **description**  |**details**|
|----------|-----------------|-----------------|------|
|31        |RDS (Relational Database Service) (45 mins)|This practical exercise will provide you with the hands-on experience necessary to support and manage RDS for large businesses. Managing RDS effectively allows your company to make strategic decisions, ensuring optimal data security, scalability, and high availability, all vital aspects for large businesses.|Set up a basic, production-aware RDS instance with high availability and backups — perfect for support engineers or system admins learning AWS-managed databases.<br>🛠️ Step-by-Step Instructions <br>🔹 Step 1: Navigate to the RDS Console  <br>Log into the AWS Management Console.  <br>Go to Services > search for "RDS" under Databases. <br>Click Create database.  <br>🔹 Step 2: Choose a Database Engine  <br>Under Engine options, choose a common engine like: <br>MySQL or PostgreSQL (for this exercise).  <br>Click Next (skip “Easy create” if it’s selected).  <br>🔹 Step 3: Configure Database Settings <br>Choose Standard Create <br>Edition: Select Free tier or Dev/Test unless production-ready sizing is needed.  <br>Set:  <br>DB instance identifier: myrds-db  <br>Master username: admin  <br>Master password: Create a secure password  <br>🔹 Step 4: Instance Configuration  <br>DB instance class: Use a small one for testing (e.g., db.t3.micro).  <br>Storage: Leave default settings (General Purpose SSD).  <br>For production: Point out where to enable Multi-AZ deployment (but SKIP enabling it now to save time).  <br>🔹 Step 5: Configure Networking  <br>Choose a VPC and subnet group (leave defaults if unsure).  <br>Ensure Public access is enabled for this lab (not for prod).  <br>VPC security group: Create new or choose existing (allow MySQL port 3306 or PostgreSQL 5432).   <br> 🔹 Step 6: Enable Backups and Monitoring  <br>Backup retention period: Set to 7 days  <br>Enable:  <br>Auto minor version upgrade  <br>Monitoring (optional for workshop)  <br>🔹 Step 7: Additional Settings <br>Database name: Optional  <br>Authentication: Keep default password authentication  <br>Encryption: Leave enabled (uses KMS by default <br>Backups and maintenance: Accept defaults for now  <br>✅ Step 8: Launch the Database  <br>Review all configurations  <br>Click Create database <br>It may take ~10 minutes to provision — use this time to:  <br>Discuss use cases  <br>Review RDS features  <br>Show how to connect using a DB client  <br>💡 Optional Discussion Topics (Time-Permitting)  <br>High Availability with Multi-AZ deployments  <br>Read Replicas for scaling reads <br>IAM-based authentication  <br>Snapshots vs automated backups  <br>Cost considerations and right-sizing|
|32       |-----------------|----------------|🖥️Step 7: Launch EC2 Instance for Database Access<br>Go to EC2 Dashboard → Click Launch instance.<br>Use Amazon Linux 2 (for MySQL) or Ubuntu (for PostgreSQL).<br>Ensure the instance is launched in the same VPC and subnet as your RDS instance.<br>Under Security Group settings:<br>Allow outbound internet access.<br>In RDS's security group, allow inbound traffic from this EC2 (on port 3306 for MySQL or 5432 for Postgres).<br>Once launched, connect via SSH to the EC2 instance.<br>🧰Step 8: Install Database Client and Connect to RDS<br>🔧 Install MySQL or PostgreSQL Client<br>Amazon Linux:<br>sudo yum install mysql -y<br>Ubuntu:<br>sudo apt-get update && sudo apt-get install postgresql-client -y<br>🔌 Connect to Your RDS Instance<br>Get your RDS endpoint from the RDS Console (under “Connectivity”).<br>For MySQL:<br> -h <your-RDS-endpoint> -u admin -p<br>For PostgreSQL:<br> -h <your-RDS-endpoint> -U admin -d postgres<br>Enter your password when prompted. <br>🗄️Step 9: Run Basic SQL Commands<br>✅ Create a Table<br>MySQL:<br>CREATE TABLE employees (   <br> id INT PRIMARY KEY AUTO_INCREMENT,  <br>  name VARCHAR(100),   <br> role VARCHAR(100)<br>);<br>PostgreSQL: use SERIAL instead of AUTO_INCREMENT.<br>CREATE TABLE employees (  <br>  id SERIAL PRIMARY KEY,  <br>  name VARCHAR(100),  <br>  role VARCHAR(100)<br>);<br>✅ Insert Data<br>INSERT INTO employees (name, role) VALUES ('John Doe', 'Software Engineer');<br>INSERT INTO employees (name, role) VALUES ('Jane Smith', 'Data Analyst');<br>✅ Query Data<br>SELECT * FROM employees;<br>✅ Update a Record<br>UPDATE employees SET role='Senior Engineer' WHERE name='John Doe';<br>✅ Delete a Record<br>DELETE FROM employees WHERE name='Jane Smith';<br>🛡️ Step 10: Backups & Snapshots<br>In the RDS Console, go to Snapshots.<br>Click Take snapshot (this is a manual snapshot).<br>Discuss:<br>Automated backups (scheduled, auto-deleted).<br>Manual snapshots (you control retention).📊 Step 11: Monitoring<br>In the RDS Console, click your DB instance.<br>Go to the Monitoring tab.<br>Review key metrics:<br>CPU Utilization<br>Free Storage Space<br>Database Connections<br>Optionally, set up a CloudWatch Alarm (for alerting).<br>📈 Step 12: Scaling<br>Go to Modify DB instance.<br>Possible changes<br>Instance class (vertical scaling).<br>Allocated storage.<br>Enable Read Replicas (horizontal scaling for reads).<br>Apply changes immediately or during the next maintenance window.<br>🔁 Step 13: High Availability (Multi-AZ)<br>While creating or modifying a DB:<br>Enable Multi-AZ deployment.<br>AWS automatically provisions a synchronous standby in another AZ.<br>Use case: protection against AZ-level failures (automatic failover).|
|33       |Athena + S3 Querying (30–45 mins)|------|🔍 Athena & Glue Lab: Serverless Querying Over S3 Data  <br>🎯 Goals  <br>Learn how to query S3 data using Athena    <br>Understand schema discovery using AWS Glue    <br>Practice SQL on semi-structured data    <br>✅ Step 1: Prepare Mock Data  <br>Open any text editor or spreadsheet tool.    <br>Create a simple CSV file named employees.csv with the following<br>content:id,name,department,salary  <br>1,John Doe,Engineering,80000  <br>2,Jane Smith,Marketing,70000  <br>3,Bob Brown,Engineering,82000  <br>4,Lisa Ray,Finance,90000  <br>5,Tina Chen,Marketing,72000  <br>Save the file locally.  <br>☁️ Step 2: Upload to S3 <br>Navigate to the S3 Console.  <br>Create a new bucket or use an existing one.  <br>Inside the bucket, create a folder called data/ (optional but clean).  <br>Upload employees.csv into this folder.  <br>🧠 Step 3: Set Up Schema Discovery (Glue Crawler)  <br>Go to the AWS Glue Console → Click Crawlers → Click Create crawler.  <br>Name your crawler (e.g., EmployeeDataCrawler).  <br>Choose Data stores → S3 and point to the folder/bucket where the CSV is uploaded.  <br>Create a new IAM role or use an existing one.  <br>Set the crawler to run on demand.   <br>Choose to add to a new database (e.g., employee_db).  <br>Run the crawler.  <br>➡️ This creates a table schema in the Glue Data Catalog based on your CSV.  <br>  🔎 Step 4: Query with Athena  <br>Open the Athena Console.  <br>Set the Query result location under Settings (e.g., s3://your-bucket/query-results/). <br>In the editor:  <br>Choose your database (employee_db).  <br>Run a basic query:  <br>SELECT * FROM employees;  <br>📊 Step 5: Try SQL Filters & Aggregations  <br>🔽 Filter by Department  <br>SELECT * FROM employees WHERE department = 'Engineering';  <br>📈 Average Salary per Department <br>SELECT department, AVG(salary) as avg_salary  <br>FROM employees  <br>GROUP BY department;  <br>📊 Highest Salary  <br>SELECT name, salary FROM employees ORDER BY salary DESC LIMIT 1;  <br>🧼 Optional Cleanup  <br>Delete the S3 objects and bucket if no longer needed.  <br>Delete Glue table/database if testing only.  |
|34        |CHECKPOINT|-----------------|------|
|35        |🔚 Workshop Wrap-Up (15–20 mins)|- Recap key learnings  <br>- Address FAQs  <br>- Share next steps / further learning paths|------|
